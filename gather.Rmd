---
title: "Gather"
author: "Tate Huffman"
date: "2/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(baseballr)
library(tidyverse)
```

## Gather

Below is code written to gather five years of Baseball Savant data. It takes a long time to run, but the data necessary is in my [GitHub repository](https://github.com/tjhuffman4246/finalproject_initial) for this project.

```{r gather, eval = FALSE}

# code chunk setting prevents this code from actually running, given its length

# function to generate list of needed dates, given start and end

date_list <- function(start, end) {
  seq(as.Date(start), as.Date(end), by = "days")
}

# uses start and end dates of 2015-2019 MLB seasons to get lists of dates

dates_2015 <- date_list("2015-04-05", "2015-10-04")
dates_2016 <- date_list("2016-04-03", "2016-10-02")
dates_2017 <- date_list("2017-04-02", "2017-10-01")
dates_2018 <- date_list("2018-03-29", "2018-10-01")
dates_2019 <- date_list("2019-03-28", "2019-09-29")

# could just scrape data from start date to end date
# but we want the data to be in order of date played and ordered within game
# so instead we write a function to get data for each date
# and within that function we reorder the data by inning for each game
# with some help from Andy Price's fall 2019 project (GitHub below)
# https://github.com/andyprice2/final_project/blob/master/scraping_statcast.R
# TF also helped develop the code to group by game

scrape_day <- function(date) {
  scraped <- scrape_statcast_savant(start_date = date, end_date = date, 
                         player_type = "pitcher")
  
  mod_vector <- vector()
  for(i in 1:length(sort(unique(scraped$game_pk)))) {
    game <- scraped[scraped$game_pk == sort(unique(scraped$game_pk))[i], ]
    mod_vector <- rbind(mod_vector, game)
  }
  
  scraped <- as.data.frame(mod_vector)
  return(map_df(scraped, rev))
  
}

# function to get yearly data, write to csv, and then remove from envmt.
# saves processing power - had difficulty collecting all the data
# "year" line creates a string that extracts name of the passed-in dataframe
# sets environment to .GlobalEnv when removing file to ensure it's removed

write_and_remove <- function(data) {
  var <- ".csv"
  year <- deparse(substitute(data))
  write_csv(data, path = paste0("data/", paste0(year, var)))
  rm(list = as.character(substitute(data)), envir = .GlobalEnv)
}

# uses dates and scraping functions to get yearly data
# used Git LFS to upload these files
# because of processing issues we have to get data for each year...
# write it as a csv, and then read that csv file back in

data_2015 <- map_dfr(dates_2015, scrape_day)
write_and_remove(data_2015)

data_2016 <- map_dfr(dates_2016, scrape_day)
write_and_remove(data_2016)

data_2017 <- map_dfr(dates_2017, scrape_day)
write_and_remove(data_2017)

data_2018 <- map_dfr(dates_2018, scrape_day)
write_and_remove(data_2018)

data_2019 <- map_dfr(dates_2019, scrape_day)
write_and_remove(data_2019)

# creates function to load in these files, via below Stack Overflow thread:
# https://stackoverflow.com/questions/23190280/
# whats-wrong-with-my-function-to-load-multiple-csv-files-into-single-dataframe

load_files <- function(path) { 
  files <- dir(path, pattern = "data_\\d{4}.csv", full.names = TRUE)
  bind_rows(map_df(files, read_csv))
}

pitches <- load_files("data")

```

